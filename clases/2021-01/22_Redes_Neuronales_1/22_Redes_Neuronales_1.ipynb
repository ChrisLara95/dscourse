{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2787705-568d-4d9f-b316-3d26dda12bba",
   "metadata": {},
   "source": [
    "# Clase 22: Redes Neuronales\n",
    "\n",
    "**MDS7202: Laboratorio de Programación Científica para Ciencia de Datos**\n",
    "\n",
    "**Profesor: Pablo Badilla**\n",
    "\n",
    "Basado en las clases de Nicolás Caro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8140d11d-3dfb-4c84-a396-a42d5711ea26",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Introducción\n",
    "\n",
    "La redes neuronales aparecen bajo el nombre de **perceptrón**, propuesta por [Rosenblatt en 1958](http://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf). Este es un modelo de clasificación consistente en pesos $w$ y una función de output del tipo $f(w^t\\mathbf{x})$ para el input $\\mathbf{x}$. \n",
    "\n",
    "\n",
    "El percetrón implementa $f(\\cdot)$ predice en base a una función de salto, que entrega el valor 1 para argumentos mayores que 0 y retorna 0 en otro caso. \n",
    "\n",
    "El principal problema de este modelo consiste en su poca expresibilidad, pues solo permite hacer separaciones lineales. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='./resources/perceptron.png' width=600/>\n",
    "</div>\n",
    "    \n",
    "    \n",
    "    \n",
    "<div align='center'>\n",
    "Fuente:\n",
    "    <a href='https://www.javatpoint.com/single-layer-perceptron-in-tensorflow'>javatpoint</a>.\n",
    "</div>\n",
    "\n",
    "<div align='center'>\n",
    "<img src='./resources/step.png' width='400' />\n",
    "</div>\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b4479-9a7c-4ba0-9252-c7657168fffe",
   "metadata": {},
   "source": [
    "### Ejemplo a mano\n",
    "\n",
    "En este ejemplo haremos un perceptrón que clasifique si un vector representa o no a un gato.\n",
    "\n",
    "El vector está compuesto por los siguientes atributos:\n",
    "\n",
    "\\[`tiene cola`, `tiene dos orejas`, `son adoptados como mascotas`, `maulla`\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e8b05-e545-464b-aac9-abb96795ab1e",
   "metadata": {},
   "source": [
    "Generamos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88262027-a1f6-4c67-bd4d-f9467425db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "gato = [1, 1, 1, 1]\n",
    "persona = [0, 1, 0, 0]\n",
    "arbol = [0, 0, 0, 0]\n",
    "perro = [1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4a3dd10-3a3f-43f1-833c-5acab33e6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = np.array([gato, persona, arbol, perro])\n",
    "\n",
    "# solo el primero lo etiquetamos como gato.\n",
    "etiquetas = np.array([1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2afc44f-4a62-4207-b4ac-a13ede13b20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [1, 1, 1, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cf0a19f-7b63-48e0-9dc2-37f49fa6b553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2510ccbc-3d9e-4b27-9d97-7e29df7ec02f",
   "metadata": {},
   "source": [
    "Seleccionamos un ejemplo a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c04433c2-0e8f-4605-af22-79dfdb0bfbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ejemplo = perro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc12700-a90e-4f48-92ae-ddd47fa0e215",
   "metadata": {},
   "source": [
    "Definimos nuestros pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b349938-21f3-4579-ae1b-2613a05ad1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesos = np.array([0.0, -0.01, 0.0, 0.03])\n",
    "sesgo = -0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01064075-6ff9-4499-b107-6c845dba2a56",
   "metadata": {},
   "source": [
    "Calculamos la suma ponderada de los pesos por la entrada. Comunmente a esto le llamamos *acumulación de información* o *carga*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c261661-21e6-4a67-8865-26dce9cdc730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.  , -0.01,  0.  ,  0.  ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesos * ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7679e15-8b4e-4b19-8013-3f96f491d525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.02"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pesos * ejemplo) + sesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6a895f-7097-4f71-9926-5ba739efd16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.02"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(pesos, ejemplo) + sesgo  # mucho más simple usar np.dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a056293-08ec-4120-89c1-b9b73e9e3ef2",
   "metadata": {},
   "source": [
    "Noten que el sesgo define que hace el perceptrón en el caso que toda la entrada vale 0. En general, desplaza la función de entrada hacia arriba o hacia abajo.\n",
    "\n",
    "Otra forma de verlo (cuando es negativo) es cuanta información acumulada mínima requiero para activar la neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc205b6-a9cb-425d-af87-31220c0d1772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.02"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acumulacion_de_informacion = np.dot(pesos, ejemplo) + sesgo\n",
    "acumulacion_de_informacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbd78a-a4f2-410e-a7db-9c38f790e07c",
   "metadata": {},
   "source": [
    "Por último, clasificamos. \n",
    "\n",
    "- Si la suma es mayor que 0, retornamos 1.\n",
    "- Si no, retornamos 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47b278e-6424-4b3b-a73e-3c5931d6b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcion_de_activacion(x):\n",
    "    return 1 if x > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "136fd562-0d74-411f-a13e-398dd99165fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediccion = funcion_de_activacion(acumulacion_de_informacion)\n",
    "prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0130982-f823-4827-8157-433b942f317e",
   "metadata": {},
   "source": [
    "### Aprendizaje en un Perceptrón y Descenso del Gradiente\n",
    "\n",
    "Entrenar un perceptron consiste en ir ajustando los pesos a medida que vemos los datos de entrenamiento. Comunmente, esto se logra a través del descenso del gradiente.\n",
    "\n",
    "Para explicar esta idea, primero definimos una función de pérdida o loss $\\mathcal{L}$. Esta función se encarga de cuantificar cuanto nos equivocamos al predecir nuestros ejemplos de entrenamiento.\n",
    "\n",
    "El descenso del gradiente simplemente es un **método para minimizar una función de pérdida** a partir de datos de entrenamiento:\n",
    "\n",
    "$$w_{n+1} = w_{n} - \\alpha \\cdot \\Delta \\mathcal{L} $$\n",
    "\n",
    "En este caso:\n",
    "- $w_n$ representa los pesos actuales. $w_{n+1}$ los pesos actualizados.\n",
    "- $\\alpha$ se denomina como tasa de aprendizaje o Learning Rate. Es un hiperparámetro que ajusta tanto se mueven los pesos en cada iteración de entrenamiento.\n",
    "- $\\Delta\\mathcal{L}$ es el gradiente de la función de pérdida $\\mathcal{L}$ (i.e., su derivada con respecto cada parámetro)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc31584-aa4a-4dd3-a09b-8f32ac456930",
   "metadata": {},
   "source": [
    "#### Paréntesis: Tasa de Aprendizaje\n",
    "\n",
    "<div align='center'>\n",
    "<img src='./resources/learning_rates.png' width=900/>\n",
    "</div>\n",
    "\n",
    "<div align='center'>\n",
    "    Fuente: <a href='http://www.bdhammel.com/learning-rates/'>http://www.bdhammel.com/learning-rates/</a>\n",
    "</div>\n",
    "\n",
    "Veamos que sucede caso a caso. Una tasa de aprendizaje...\n",
    "\n",
    "- muy baja tomará mucho tiempo en converger.\n",
    "- alta saltará las mejores configuraciones \n",
    "- muy alta incluso podría diverger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc07686-8b44-4c10-a4a7-3f135188fe3e",
   "metadata": {},
   "source": [
    "\n",
    "#### Relacionado: Proyecciones de Funciones de Loss de Distintas Redes Neuronales\n",
    "\n",
    "<div align='center'>\n",
    "<img alt='Proyecciones de Funciones de Loss de Distintas Redes Neuronales' src='./resources/proyecciones_loss.png' />\n",
    "</div>\n",
    "    \n",
    "<div align='center'>\n",
    "    Fuente: <a href='https://www.cs.umd.edu/~tomg/projects/landscapes/'>https://www.cs.umd.edu/~tomg/projects/landscapes/</a>\n",
    "</div>\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061faa8-8070-442a-9e8b-99029a43edc7",
   "metadata": {},
   "source": [
    "> **Pregunta:** Viendo las imágenes anteriores, ¿es posible que el descenso del gradiente no encuentre el optimo global?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef8dcb2-82ef-4a29-af65-c4180326b8bb",
   "metadata": {},
   "source": [
    "#### Siguiendo con el Descenso del Gradiente\n",
    "\n",
    "\n",
    "El descenso del gradiente simplemente es un método para minimizar una función de pérdida a partir de datos de entrenamiento:\n",
    "\n",
    "$$w_{n+1} = w_{n} - \\alpha \\cdot \\Delta \\mathcal{L} $$\n",
    "\n",
    "En este caso:\n",
    "- $w_n$ representa los pesos actuales. $w_{n+1}$ los pesos actualizados.\n",
    "- $\\alpha$ se denomina como tasa de aprendizaje o Learning Rate. Es un hiperparámetro que ajusta tanto se mueven los pesos en cada iteración de entrenamiento.\n",
    "- $\\Delta\\mathcal{L}$ es el gradiente de la función de pérdida $\\mathcal{L}$ (i.e., su derivada con respecto cada parámetro)\n",
    "\n",
    "\n",
    "\n",
    "En nuestro caso, usaremos la función de pérdida *suma de los errores cuadráticos* o *sum of squared errors* **SSE**: \n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{2}\\sum_i (y - \\hat{y})^2$$\n",
    "\n",
    "Por lo tanto (suponiendo que en cada iteración solo vemos un ejemplo (i.e., omitimos la sumatoria)), su gradiente (para cualquier peso $j$) es:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{j}}\\mathcal{L} = y - \\hat{y}$$\n",
    "\n",
    "\n",
    "Así, el descenso del gradiente para entrenar nuestro perceptrón es:\n",
    "\n",
    "$$w_{n+1} = w_{n} - \\alpha \\cdot (y - \\hat{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d952c-3517-4c95-b520-e1940618848d",
   "metadata": {},
   "source": [
    "Para ejemplificar lo dicho anteriormente, implementaremos un perceptrón:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b10e3ed0-c902-4958-a971-a7d37e46a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basado en # https://medium.com/@thomascountz/19-line-line-by-line-python-perceptron-b6f113b161f3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def funcion_de_activacion(x):\n",
    "    return 1 if x > 0 else 0\n",
    "\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self, dimensiones_input, epocas=100, tasa_aprendizaje=0.01):\n",
    "        self.epocas = epocas\n",
    "        self.tasa_aprendizaje = tasa_aprendizaje\n",
    "        # los pesos los guardamos como un arreglo que contiene el sesgo en el índice 0 y\n",
    "        # los pesos que multiplican al input desde el índice 1.\n",
    "        self.pesos = np.zeros(dimensiones_input + 1)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        acumulacion_de_informacion = np.dot(inputs, self.pesos[1:]) + self.pesos[0]\n",
    "        return funcion_de_activacion(acumulacion_de_informacion)\n",
    "\n",
    "    def fit(self, datos_entrenamiento, etiquetas):\n",
    "        # Una época es una pasada completa sobre el dataset de entrenamiento.\n",
    "        for _ in range(self.epocas):\n",
    "            for entrada, etiqueta in zip(datos_entrenamiento, etiquetas):\n",
    "                prediccion = self.predict(entrada)\n",
    "                self.pesos[1:] += (\n",
    "                    self.tasa_aprendizaje * (etiqueta - prediccion) * entrada\n",
    "                )\n",
    "                self.pesos[0] += self.tasa_aprendizaje * (etiqueta - prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41e184a8-cc94-4eae-a0f0-ed4e53776ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1acf8607-a7c1-40bf-b3ce-91ecc36f281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.fit(entradas, etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f48c593-0191-40cc-87ef-453086e86c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.predict(gato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23ed00be-f72f-47c9-9337-5a15ceda0c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.predict(perro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "537d246a-c2a0-45da-ba0c-e69c0e34f559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01,  0.  , -0.01,  0.  ,  0.03])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a311b-6b35-4066-b33a-d6acfb695784",
   "metadata": {},
   "source": [
    "> **Nota**: Una época es una pasada completa sobre el dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17f01c-c39a-4468-9d2b-a0a268092ac4",
   "metadata": {},
   "source": [
    "> **Pregunta:** ¿Cuales son los hiperparámetros de nuestro perceptrón? ¿Podemos variar la función de pérdida? ¿Podemos también variar la forma en que entrenamos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a74dce-3602-4f87-bb2a-ea7b7f5dd3ae",
   "metadata": {},
   "source": [
    "### Cantidad de datos que le entregamos al Descenso del Gradiente:\n",
    "\n",
    "El entrenamiento por medio del descenso del gradiente puede variar según la cantidad de datos que le entregamos en cada iteración. A continuación se muestran 3 enfoques:\n",
    "\n",
    "\n",
    "- Descenso del gradiente en lotes (o **batch**): En este caso entregamos todos los datos en una sola pasada. Puede estancar el aprendizaje debido a que siempre usaremos todas las muestras.\n",
    "\n",
    "\n",
    "- Descenso del Gradiente Estocástico (Stochastic Gradient Descent o **SGD**): Usamos una muestra aleatoria en cada iteración. El gradiente y la actualización de pesos se calcula en relación a esa muestra en particular, lo que dificulta el estancamiento. Sin embargo, implica lentitud en el proceso.\n",
    "\n",
    "- Descenso del gradiente Estocástico en mini-lotes (**mini-batch**): Mezcla entre las opciones anteriores. Ingresa un pequeño batch y actualiza los pesos sobre el gradiente calculado a partir de ese batch. Los ejemplos del batch son elegidos aleatoriamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59344092-8264-42d4-8d7f-303fdad4a8db",
   "metadata": {},
   "source": [
    "> **Pregunta**: ¿Por qué se usa descenso de gradiente y no otros métodos de optimización (como derivar e igualar a cero...)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397500a5-2231-4258-a929-4f2f593e613a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Perceptrón Multicapa\n",
    "\n",
    "\n",
    "Sobre el modelo anterior, podemos construir uno más complejo llamado *perceptron multicapa* o *MultiLayer Perceptron* (**MLP**). Un MLP consiste en múltiples modelos de perceptron intecomunicadas. Cada uno de estos modelos se denota como *unidad* (*unit*) y se organizan en capas secuenciales.\n",
    "\n",
    "<div align='center'>\n",
    "<img alt='MLP esquema simple' src='./resources/mlp_simple.png' width=600/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div align='center'>\n",
    "    Fuente: <a href='https://becominghuman.ai/multi-layer-perceptron-mlp-models-on-real-world-banking-data-f6dd3d7e998f/'>https://becominghuman.ai/multi-layer-perceptron-mlp-models-on-real-world-banking-data-f6dd3d7e998f/</a>\n",
    "</div>\n",
    "    \n",
    "\n",
    "En la figura:\n",
    "\n",
    "- La *capa input* corresponde a un conjunto de perceptrones que operan sobre el vector de entrada que distribuyen los valores del vector a la siguiente capa.\n",
    "\n",
    "- Las siguientes capas se denotan como *capas ocultas* y se contruyen de manera análoga a la capa input (o inicial). La primera capa oculta consiste en un conjunto de percetrones que operan sobre el output de la capa input. Sobre esta primera capa oculta pueden haber multiples capas ocultas que operan sobre el output de la capa oculta anterior. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172188a-da21-4ead-a85a-84f2935bc39b",
   "metadata": {},
   "source": [
    "#### Propagación Hacia Adelante o *Feedforward*\n",
    "\n",
    "\n",
    "El proceso de generar una salida sobre la entrada de una capa anterior se denota *propagación hacia adelante* (*feedforward*) la propagación termina en una última capa denominada *capa output* que entrega el resultado final de la clasificación. \n",
    "\n",
    "Bajo el punto de vista del aprendizaje automático, las capas ocultas de un perceptron multicapa (red neuronal) generan abstracciones o características a partir de los datos sobre los cuales operan. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c896644-3964-47cc-b6e5-4279f63232f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `Pytorch`\n",
    "\n",
    "<div align='center'>\n",
    "<img alt='MLP esquema simple' src='./resources/pytorch_logo.png' width=600/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Para trabajar con redes neuronales haremos uso de la librería **`Pytorch`**. Esta librería consiste en un conjunto de herramientas diseñadas para generar modelos basados en redes neuronales utilizando las capacidades de computo distribuido que ofrecen las GPU (unidades de procesamiento gráfico / tarjetas de video). Esto permite operaciones de vectorización aceleradas y distribuidas. Esta librería se importa como `torch`.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se implementa una red neuronal simple utilizando Pytorch. Para esto, se importa el módulo y se indica una semilla aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e87b5f70-bd8a-4c7b-abe8-2f2a13f16c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd72a2fa770>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Se asigna un valor de reproductibilidad\n",
    "torch.manual_seed(6202)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efdb149-521b-4b75-93cd-d7b9c03f9334",
   "metadata": {},
   "source": [
    "#### Dataset de Ejemplo: Vino 🍷\n",
    "\n",
    "Se carga el dataset sobre el cual trabajaremos, en este caso será un conjunto de datos de vinos. Este conjunto consta de 13 atributos continuos, cada vino posee un identificador dentro de 3 clases. La idea es asignar un tipo de vino a cada observación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45c9d4a8-b7ed-4275-9c23-9dd0dbe67c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0      1    14.23        1.71  2.43               15.6        127   \n",
       "1      1    13.20        1.78  2.14               11.2        100   \n",
       "2      1    13.16        2.36  2.67               18.6        101   \n",
       "3      1    14.37        1.95  2.50               16.8        113   \n",
       "4      1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = [\n",
    "    \"class\",\n",
    "    \"Alcohol\",\n",
    "    \"Malic acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of diluted wines\",\n",
    "    \"Proline\",\n",
    "]\n",
    "\n",
    "wine_data = pd.read_csv(\n",
    "    \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\",\n",
    "    names=names,\n",
    ")\n",
    "\n",
    "wine_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f38cc7-0ed8-4a88-8520-b467e9037a20",
   "metadata": {},
   "source": [
    "Se procede a hacer una separación en entrenamiento y test, se hará es una codificación dummy para la variable de respuesta y se estandarizan las variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1521b84a-6a79-4b5e-8767-ba816365ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "response = [\"class\"]\n",
    "num_cols = wine_data.loc[:, \"Alcohol\":].columns.values\n",
    "\n",
    "data_transform = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"normaliza\", StandardScaler(), num_cols),\n",
    "        (\"codifica\", OrdinalEncoder(), response),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e500e-c5ee-461f-93d4-1d7a8363fd4b",
   "metadata": {},
   "source": [
    "Para evitar fuga de información, se hace una separación en train y test para luego transformar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d0036d6-745e-433f-8778-e909618fd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test = train_test_split(wine_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2336fb10-89a6-4a3d-bd6b-d75f5c517d67",
   "metadata": {},
   "source": [
    "Se aplica el preprocesamiento sobre el conjunto train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "415bf9b0-b631-4aa6-9cfd-7f97edcd2ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_transform.fit_transform(data_train)\n",
    "\n",
    "# Se obtienen las variables numericas y de respuesta\n",
    "\n",
    "X_train = data[:, :-1].copy()\n",
    "y_train = data[:, -1:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "947c48fa-8781-4766-973e-093a4dda442f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 13)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1258f11-6650-4a29-9b4e-70cba81f8868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d53fa7-5d8e-45e7-a3bc-10dae80651d5",
   "metadata": {},
   "source": [
    "#### MLP en `Pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513a481-bb23-4163-a61d-30bd8f770909",
   "metadata": {},
   "source": [
    "El perceptron multicapa que crearemos consiste en 10 capas ocultas y su función de activación tiene la forma:\n",
    "$$f(x) = max(0,x)$$\n",
    "\n",
    "Esta es una función de activación conocida y se denota como **ReLU (Rectified Linear Unit)**. \n",
    "\n",
    "Se definen los parámetros, para la función de activación se utiliza el módulo de redes neuronales de Pytorch, se puede acceder a este módulo por medio de `torch.nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f9c4b0c-c6ad-476c-99be-b57ed881f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero, definimos el tamaño de la  capa de entrada que es igual al tamaño del vector de la entrada\n",
    "# y el tamaño de la salida, que es igual al número de clases.\n",
    "input_dim, output_dim = (X_train.shape[1], 3)\n",
    "\n",
    "# Equivalente al output de la capa input\n",
    "input_dim_capas_ocultas = 5\n",
    "\n",
    "# por último definimos la función de activación.\n",
    "f_activation = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1dda06-1f6f-4818-a91d-eab45164beb0",
   "metadata": {},
   "source": [
    "#### Capa de Entrada\n",
    "\n",
    "Para definir la primera capa, tenemos que tener en cuenta que buscamos una transformación de la forma $x \\mapsto w^t x$ que luego es clasificada por medio de $f(w^t x)$. Esto quiere decir que los parámetros a entrenar serán los vectores de peso $w$ asociados a cada unidad (perceptron) de cada capa (input, ocultas y output).\n",
    "\n",
    "`Pytorch` ofrece una abstracción para capas formadas por perceptrones de la forma $f(w^t x)$. Esta abstracción es la clase `Linear` (observe que $x \\mapsto w^t x$ es una transformación lineal).\n",
    "\n",
    "A continuación definimos la capa de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9459b4f-eeb2-4820-8d95-399c8a99004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "# in_features: tamaño del vector de entrada.\n",
    "# out_features: tamaño de la primera capa oculta.\n",
    "capa_input = Linear(in_features=input_dim, out_features=input_dim_capas_ocultas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146b483-4c40-4bd6-95f0-2518787369df",
   "metadata": {},
   "source": [
    "#### Capas Ocultas\n",
    "\n",
    "Luego se definen las capas ocultas, estas también corresponden a transformaciones lineales sobre sus capas predecesoras. En este caso, cada capa oculta tendrá dimensión 5 como input y output. A continuación, definimos como ejemplo una capa oculta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ac6b14e-6eac-40a2-bf01-02f49722edbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=5, bias=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear(input_dim_capas_ocultas, input_dim_capas_ocultas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d74238-6f9d-47c7-877a-1f163e132098",
   "metadata": {},
   "source": [
    "### Arquitectura de la Red\n",
    "\n",
    "\n",
    "La arquitectura de esta red es de naturaleza secuencial y corresponde al siguiente algoritmo:\n",
    "\n",
    "1. Input: x de dimensión 13\n",
    "2. Opera x en cada unidad de la capa lineal input, es decir, calcula $w_i^t x$ para $i = 1, \\ldots, 5$ (dimensión de salida de la capa input).\n",
    "3. Opera RelU($w_i^t x$) para $i = 1, \\ldots, 5$, se obtiene un vector de dimensión 5. \n",
    "\n",
    "4. Para cada una de las 2 capas ocultas y de manera secuencial:\n",
    "    1. Genera una transformación lineal sobre los outputs de la capa anterior.\n",
    "    2. Aplica ReLU sobre las transformaciones lineales.\n",
    "    3. En la última capa oculta el vector resultante se pasa a la capa output.\n",
    "\n",
    "5. Se recibe el resultado de la última capa oculta, se transforma de manera lineal sobre 3 perceptrones para finalmente ser transformada por medio de una función softmax (dimensión del output es 3). \n",
    "\n",
    "Esto se reduce a: \n",
    "```\n",
    "X  -> capa input -> ReLU() -> Co_1 -> ReLU() -> Co_2 -> ReLU() -> capa output -> predicción\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fbd55-e727-49a4-9254-7fcb8e4c506e",
   "metadata": {},
   "source": [
    "Para programar el esquema anterior se hace uso de un diccionario ordenado `OrderedDict` de la librería `collections`. Este tipo de objetos opera de manera similar a las Pipelines de Scikit-learn, pues reciben un conjunto de tuplas del tipo `(identificador,objeto)`. Se procede a generar la arquitectura de la red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bf8958f-e758-4ab2-b13b-4e78383946d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('capa input', Linear(in_features=13, out_features=5, bias=True)),\n",
       "             ('capa oculta_0',\n",
       "              Linear(in_features=5, out_features=5, bias=True)),\n",
       "             ('relu_0', ReLU()),\n",
       "             ('capa oculta_1',\n",
       "              Linear(in_features=5, out_features=5, bias=True)),\n",
       "             ('relu_1', ReLU()),\n",
       "             ('capa ouput', Linear(in_features=5, out_features=3, bias=True)),\n",
       "             ('Softmax', Softmax(dim=1))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "n_capas_ocultas = 2\n",
    "\n",
    "# capa input\n",
    "input_ = (\"capa input\", capa_input)\n",
    "relu = (\"relu\", f_activation)\n",
    "\n",
    "# etapas (similar al pipeline)\n",
    "steps = [input_]\n",
    "\n",
    "# capas ocultas\n",
    "for i in range(n_capas_ocultas):\n",
    "\n",
    "    capa_i = (\n",
    "        f\"capa oculta_{i}\",\n",
    "        Linear(input_dim_capas_ocultas, input_dim_capas_ocultas),\n",
    "    )\n",
    "    relu_i = (f\"relu_{i}\", f_activation)\n",
    "\n",
    "    steps += [capa_i, relu_i]\n",
    "\n",
    "# capa output\n",
    "output = (\"capa ouput\", Linear(input_dim_capas_ocultas, output_dim))\n",
    "\n",
    "steps.extend([(output), (\"Softmax\", torch.nn.Softmax(dim=1))])\n",
    "\n",
    "# Se utiliza la estructura de diccionario ordenado\n",
    "steps = OrderedDict(steps)\n",
    "\n",
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b1ab7-ff74-420e-9a9b-ea50f01322c5",
   "metadata": {},
   "source": [
    "Luego, cuando ya se posee la arquitectura, se inicializa un objeto `Sequential` del módulo `nn`, este objeto permite modelar una red neuronal multicapa recibiendo como input los componentes de la arquitectura de manera ordenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66e16432-5fe9-40cb-bff4-7b487999191e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (capa input): Linear(in_features=13, out_features=5, bias=True)\n",
       "  (capa oculta_0): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (relu_0): ReLU()\n",
       "  (capa oculta_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (relu_1): ReLU()\n",
       "  (capa ouput): Linear(in_features=5, out_features=3, bias=True)\n",
       "  (Softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import Sequential\n",
    "\n",
    "# MLP -> multi layer perceptron\n",
    "mlp = Sequential(steps)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b84e66-bb10-43ef-8fb7-ad277603130a",
   "metadata": {},
   "source": [
    "El resultado entregado por la capa output corresponde a un vector de tres dimensiones, se asigna la clase predicha a aquella componente con el mayor valor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e053b-5f97-44cb-9ea9-fae978643582",
   "metadata": {},
   "source": [
    "### Funciones de Pérdida y Entropía Cruzada\n",
    "\n",
    "Luego de definir la red, es necesario definir la función de perdida. Al igual que las funciones de activación, también tenemos una serie de funciones de pérdida recomendadas para ciertos problemas en particular.\n",
    "\n",
    "<div align='center'>\n",
    "<img alt='MLP esquema simple' src='./resources/tipos_de_funciones_loss.png' width=600/>\n",
    "</div>\n",
    "\n",
    "<div align='center'>\n",
    "Fuente: <a href='https://towardsdatascience.com/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8'>https://towardsdatascience.com/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8</a>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "En este caso se utiliza la entropía cruzada. Este criterio permite comparar dos distribuciones de probabilidad $q$ (aproximación) y $p$ (real) en términos de la diferencia de información esperada (en bits por ejemplo) al utilizar la distribución $q$ para describir un eventos codificados, optimizados para $p$. Dado que se trabaja en un problema de clasificación (supervisado) se conoce la distribución real $p$ para una etiqueta  (ej: etiqueta (1,0,0), distribución (100%, 0, 0) ), por otra parte, la distribución aproximada viene dada por nuestro modelo. La entropía cruzada entre $p$ y $q$ se expresa según:\n",
    "$$\n",
    "H(p, q)=-\\sum_{x \\in \\mathcal{X}} p(x) \\log q(x)\n",
    "$$\n",
    "\n",
    "Se implementa mediante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52ddd04a-9ec1-4eb6-9bc5-f4015bcd9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterio = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7138327-90dc-4ac0-8627-dc46be7db422",
   "metadata": {},
   "source": [
    "Se debe seleccionar el optimizador a utilizar, en este caso será **descenso de gradiente estocástico**.  Se inicializa entregando dichos parámetros y los coeficientes sobre los que opera, en este caso, los parámetros de la red `mpl` a los cuales se acceede por medio del método `.parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea0b91d8-3574-4492-87e7-fd6d992324e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizador = torch.optim.SGD(mlp.parameters(), lr=0.05, momentum=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35567c6e-8fac-4b51-a4dc-c30ddee2f47f",
   "metadata": {},
   "source": [
    "Finalmente, se puede entrenar la red definida, para ello se define una cantidad de *épocas* (*epochs*), esto se refiere a la cantidad de veces que se entrena utilizando el conjunto de entrenamiento. Este proceso tiene el siguiente orden:\n",
    "\n",
    "1. Genera un conjunto de inputs en un formato compatible.\n",
    "2. En cada época:\n",
    "    1. Inicializa los gradientes asociados al optimizador, esto evita que se acumulen gradientes entre épocas.\n",
    "    2. Se opera sobre los inputs para obtener las predicciones.\n",
    "    3. Se calcula la función de loss.\n",
    "    4. Se calcula el gradiente para cada parámetro. Este proceso se denomina como *propagación hacia atrás* (*backpropagation*).\n",
    "    5. Se actualizan los parámetros según los gradientes usados.\n",
    "    \n",
    "Se implementa el esquema anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d67185ad-a8c4-4d8c-8dd5-019bcba5423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso A - Inicializa los gradientes asociados al optimizador\n",
    "datos_input = torch.autograd.Variable(torch.Tensor(X_train))\n",
    "labels = torch.autograd.Variable(torch.Tensor(y_train.reshape([-1,])).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0bfec8dc-e035-4dd1-804c-2e54cacbd294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([142, 13])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "346a371f-dd8f-408e-a453-2a3675f208c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([142])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "384e2c83-33df-494c-a43b-470bc622c61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca:  10 Loss:  tensor(0.5520)\n",
      "Epoca:  20 Loss:  tensor(0.5520)\n",
      "Epoca:  30 Loss:  tensor(0.5519)\n",
      "Epoca:  40 Loss:  tensor(0.5519)\n",
      "Epoca:  50 Loss:  tensor(0.5519)\n",
      "Epoca:  60 Loss:  tensor(0.5518)\n",
      "Epoca:  70 Loss:  tensor(0.5518)\n",
      "Epoca:  80 Loss:  tensor(0.5518)\n",
      "Epoca:  90 Loss:  tensor(0.5518)\n",
      "Epoca:  100 Loss:  tensor(0.5518)\n"
     ]
    }
   ],
   "source": [
    "epocas = 100\n",
    "for ep in range(epocas):\n",
    "    # Paso A - Reiniciar los gradientes almacenados en el caso que existan\n",
    "    optimizador.zero_grad()\n",
    "    for i in range(20):\n",
    "        # Paso B - Calcular etiquetas según los pesos actuales\n",
    "        out = mlp(datos_input)\n",
    "        out.requires_grad_(True)\n",
    "        \n",
    "        # Paso C - Se calcula la función de loss.\n",
    "        loss = criterio(out, labels)\n",
    "\n",
    "        # Paso D - Ejecutar Backpropagation: Se calcula cuanto debe cambiar cada parámetro.\n",
    "        # según el valor de la loss.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Paso E - Se actualizan los parámetros.\n",
    "        optimizador.step()\n",
    "\n",
    "    if ((ep + 1) % 10) == 0:\n",
    "        print(\"Epoca: \", ep + 1, \"Loss: \", loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7619d-f0d2-4047-b56e-068881912f80",
   "metadata": {},
   "source": [
    "Se estudia el rendimiento en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f980596f-e9d7-40e8-804b-fa1a60e19851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte train : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        47\n",
      "         1.0       1.00      1.00      1.00        58\n",
      "         2.0       1.00      1.00      1.00        37\n",
      "\n",
      "    accuracy                           1.00       142\n",
      "   macro avg       1.00      1.00      1.00       142\n",
      "weighted avg       1.00      1.00      1.00       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dt_test = data_transform.transform(data_test)\n",
    "\n",
    "X_test = dt_test[:, :-1]\n",
    "y_test = dt_test[:, -1:]\n",
    "\n",
    "f = lambda x: torch.argmax(mlp(x), dim=1)\n",
    "\n",
    "# Train error\n",
    "datos_input = torch.Tensor(X_train).float()\n",
    "preds_train = f(datos_input)\n",
    "\n",
    "print(\"Reporte train : \\n\", classification_report(y_train.reshape([-1,]), preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a74f3ff-2752-4b7d-850e-499e73d7957d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte test : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        12\n",
      "         1.0       1.00      1.00      1.00        13\n",
      "         2.0       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Error\n",
    "datos_input = torch.Tensor(X_test).float()\n",
    "preds_test = f(datos_input)\n",
    "\n",
    "print(\"Reporte test : \\n\", classification_report(y_test.reshape([-1,]), preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7f35f-53ba-44f1-b7dc-6adaa2f5abc6",
   "metadata": {},
   "source": [
    "Viendo estos resultados, se puede decir que el clasificador entrenado fue un éxito."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73bf7d3-b51b-41f9-92d2-d0c58b5bb833",
   "metadata": {},
   "source": [
    "Las redes neuronales pueden ser descritas como un modelo matemático de procesamiento de información. En general, una red neuronal puede considerarse como un sistema con las siguientes características:\n",
    "\n",
    "1. El procesamiento de la información ocurre en unidades llamadas neuronas.\n",
    "2. Las neuronas están conectadas e intercambian información (o señales) por medio de sus conexiones.\n",
    "3. Las conexiones entre neuronas pueden ser fuertes o débiles, dependiendo de como se procesa la información.\n",
    "4. Cada neurona tiene un estado interno determinado por todas las conexiones que posee.\n",
    "5. Cada neurina tiene una función de activación que opera sobre su estado, esta función determina la información que se comparte a otras neuronas.\n",
    "\n",
    "En términos operativos, una red neuronal posee una **arquitectura** que describe el conjunto de conexiones entre neurona y un proceso de **aprendizaje** asociado, que describe el proceso entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad14b1-142c-4412-b632-fbedc26aef0f",
   "metadata": {},
   "source": [
    "Las **neuronas** por tanto, pueden ser definidas por medio de la siguiente relación:\n",
    "$$\n",
    "y=f\\left(\\sum_{i} x_{i} w_{i}+b\\right)\n",
    "$$\n",
    "\n",
    "Acá se hace el calculó $w^t x + b = \\sum_i x_i w_i + b$ sobre los inputs $x_i$ y los pesos $w_i$. Estos últimos, son valores numéricos que representan las conexiones entre neuronas, el peso $b$ se denomina *bias*. Luego se calcula el resultado de aplicar la función de activación $f(\\cdot)$. Existen distintos tipos de funciones de activación dentro de estas se pueden nombrar:\n",
    "\n",
    "* $f(x)=x$ la función identidad.\n",
    "* $f(x)=\\left\\{\\begin{array}{l}1 \\text { if } x \\geq 0 \\\\ 0 \\text { if } x<0\\end{array}\\right.$ la función de activación de umbral.\n",
    "* $f(x)=\\frac{1}{1+\\exp (-x)}$ la función sigmoide logistica, es una de las más utilizadas.\n",
    "* $f(x) =\\frac{1-\\exp (-x)}{1+\\exp (-x)}$ la función sigmoide bipolar, esta corresponde a una sigmoide escalada a $(-1,1)$.\n",
    "* $f(x) = \\frac{1-\\exp (-2 x)}{1+\\exp (-2 x)}$ la tangente hiperbólica. \n",
    "\n",
    "* $f(x)=\\left\\{\\begin{array}{l}x\\text { if } x \\geq 0 \\\\ 0 \\text { if } x<0\\end{array}\\right.$ La función de activación ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1f660-6374-44dc-a962-68584bc751ce",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Existen variantes de la función de activación ReLU, investigue al menos 3 y compare sus diferencias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
